<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=dpiI8CyVsrzWsJLBFKehGpLhv3qFjX7dUn1mYxfCXhI');ol{margin:0;padding:0}table td,table th{padding:0}.c6{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c2{orphans:2;widows:2;height:11pt}.c0{color:#0000ff;font-weight:400}.c3{orphans:2;widows:2}.c4{text-indent:36pt}.c5{page-break-after:avoid}.c1{font-weight:400}.title{padding-top:0pt;color:#000000;font-size:21pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:13pt;padding-bottom:10pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:10pt;color:#980000;font-size:16pt;padding-bottom:0pt;font-family:"Roboto";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:0pt;color:#980000;font-weight:700;font-size:13pt;padding-bottom:0pt;font-family:"Roboto";line-height:1.15;orphans:2;widows:2;text-align:left}h3{padding-top:0pt;color:#666666;font-weight:700;font-size:12pt;padding-bottom:0pt;font-family:"Roboto";line-height:1.15;orphans:2;widows:2;text-align:left}h4{padding-top:8pt;color:#666666;text-decoration:underline;font-size:11pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:8pt;color:#666666;font-size:11pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:8pt;color:#666666;font-size:11pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c6"><h1 class="c3 c5" id="h.k57pts443wjp"><span class="c1">P7 A/B Testing</span></h1><p class="c3"><span class="c1">By: Darren Liu</span></p><p class="c2"><span class="c1"></span></p><h1 class="c3 c5" id="h.h5scg48r8tew"><span class="c1">Experiment Design</span></h1><h2 class="c3" id="h.ur1kt3v5q7l8"><span class="c1">Metric Choice</span></h2><p class="c3"><span class="c0">Number of cookies and number of clicks are chosen as the invariant metrics. </span><span class="c1">Invariant metrics serve as &ldquo;sanity checks&rdquo;, metrics that one would expect to be the same in both the control group and experimental group. Checking these metrics allow us to confirm that the conditions of the experiment are valid. If the tests fail, the experiment should be halted until the reasons for the discrepancy are found and fixed.</span></p><p class="c2"><span class="c1"></span></p><p class="c3"><span class="c1">There is a high chance other metrics will not be invariant. Number of user-ids should be lower for the experimental group since visitors are likely discouraged to enroll. Gross conversion, retention, and net conversion are all ratios of user-ids and are affected for the same reason. Click-through-probability is valid, but given it is actually just a ratio of our two invariant metrics, it would be redundant to test as well.</span></p><p class="c2"><span class="c1"></span></p><p class="c3"><span class="c0">Gross conversion and net conversion are chosen as the evaluation metrics. </span><span class="c1">Evaluation metrics should measure rate of success between control and experiment groups on the hypothesis of interest. In this case, whether a given change will impact user decision to enroll and remain enrolled. Gross conversion is expected to decrease or stay the same, since the tested feature is meant as a deterrent for students unprepared for the workload. Net conversion is expected to increase or stay the same, since the feature should have filtered out students that are more prone to quitting midway.</span></p><p class="c2"><span class="c1"></span></p><p class="c3"><span class="c1">Number of pageviews and clicks are invariant and not considered. User-ids are raw counts and can be not be compared fairly if pageviews are not equal. Click-through-probability is not valid since the feature being tested comes after the click. Retention rate is a valid evaluation metric but is found to have unrealistic sample size requirements.</span></p><p class="c2"><span class="c1"></span></p><h2 class="c3" id="h.ex7wuw87um13"><span class="c1">Measuring Standard Deviation</span></h2><p class="c3"><span class="c0">Gross conversion - </span><span class="c0">0.0202</span></p><p class="c3"><span class="c0">Net conversion - 0.0156</span></p><p class="c2"><span class="c1"></span></p><p class="c3"><span class="c1">The analytic estimate should do a decent job of representing empirical variability. This is because the population of interest is unlikely to have changed substantially and should have the same distribution parameters. However, it is always important to keep in mind data collected at any point is a sample snapshot of the underlying data generating model of that time. Unobservable state changes may affect distribution parameters. That considered, there is no evidence that such a state change took place. Therefore, we expect analytic variability to match empirical variability.</span></p><p class="c2"><span class="c1"></span></p><h2 class="c3" id="h.bx5ntddleyt7"><span class="c1">Sizing</span></h2><h3 class="c3" id="h.fx5az9wbhsak"><span class="c1">Number of Samples vs. Power</span></h3><p class="c3"><span class="c0">Bonferroni correction is not used. 685,325 pageviews are needed for proper experiment setup.</span></p><p class="c2"><span class="c1"></span></p><h3 class="c3" id="h.uy2xamy5nbp"><span class="c1">Duration vs. Exposure</span></h3><p class="c3"><span class="c0">100% of traffic are diverted, requiring 18 days to complete.</span></p><p class="c2"><span class="c1"></span></p><p class="c3"><span class="c1">All traffic are included because the experiment is assessed as unrisky. Some examples that may give rise to concern include system failure, damage of company or user reputation, or exposure of sensitive information. None of these cases would reasonably apply to this experiment. </span></p><p class="c2"><span class="c1"></span></p><h1 class="c3 c5" id="h.yry1zu8g8az7"><span class="c1">Experiment Analysis</span></h1><h2 class="c3" id="h.cizdts6ye33u"><span class="c1">Sanity Checks</span></h2><p class="c3"><span class="c0">Number of cookies:</span></p><p class="c3 c4"><span class="c0">Interval - (0.4988, 0.5012)</span></p><p class="c3 c4"><span class="c0">Observed - 0.5006</span></p><p class="c3 c4"><span class="c0">Passes - True</span></p><p class="c3"><span class="c0">Number of clicks:</span></p><p class="c3 c4"><span class="c0">Interval - (0.4959, 0.5041)</span></p><p class="c3 c4"><span class="c0">Observed - 0.5005</span></p><p class="c3 c4"><span class="c0">Passes - True</span></p><p class="c2"><span class="c1"></span></p><p class="c2"><span class="c1"></span></p><h2 class="c3" id="h.p5issp8oaf4a"><span class="c1">Result Analysis</span></h2><h3 class="c3" id="h.52n1ah20cmce"><span class="c1">Effect Size Tests</span></h3><p class="c3"><span class="c0">Gross conversion:</span></p><p class="c3 c4"><span class="c0">Interval - (0.0291, -0.012)</span></p><p class="c3 c4"><span class="c0">Statistical significance - True</span></p><p class="c3 c4"><span class="c0">Practical significance - True</span></p><p class="c3"><span class="c0">Net conversion:</span></p><p class="c3 c4"><span class="c0">Interval - (-0.0116, 0.0019)</span></p><p class="c3 c4"><span class="c0">Statistical significance - False</span></p><p class="c3 c4"><span class="c0">Practical significance - False</span></p><p class="c2"><span class="c0"></span></p><h3 class="c3" id="h.clnogzxymvt2"><span class="c1">Sign Tests</span></h3><p class="c3"><span class="c0">Gross conversion:</span></p><p class="c3"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;P-value - 0.0026</span></p><p class="c3"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Statistically significant - True</span></p><p class="c2"><span class="c0"></span></p><p class="c3"><span class="c0">Net conversion:</span></p><p class="c3 c4"><span class="c0">P-value - 0.6776</span></p><p class="c3 c4"><span class="c0">Statistically significant - False</span></p><p class="c2"><span class="c0"></span></p><h3 class="c3" id="h.ea3c918crur0"><span class="c1">Summary</span></h3><p class="c3"><span class="c1">The Bonferroni correction is not used. The reason being two metrics do not warrant the need for added complexity and stringent requirements of the Bonferroni. The results of the effect size tests and sign tests were consistent. Gross conversion is found to be lower in the experimental group than the control group. The difference in net conversion between the two groups are found to be negligible.</span></p><p class="c2"><span class="c1"></span></p><p class="c2"><span class="c1"></span></p><h2 class="c3" id="h.2iroxj5zbf41"><span class="c1">Recommendation</span></h2><p class="c3"><span class="c1">Based on the findings, the recommendation is to launch the tested feature. This is based on our rejection of the first null hypothesis, that gross conversion equals zero, with a negative confidence interval, as well as the failure to reject the second null hypothesis, that net conversion equals zero. As a result of the tested feature, l</span><span class="c1">ess people signed up for the free trial yet the same number of people made first payment. This shows the feature effectively eliminating users that were not prepared to become dedicated paying customers, without adverse effect to paying customer count. The business benefit is less resources expended on the customers of the 14 day trial and a better retention rate.</span></p><p class="c2"><span class="c1"></span></p><h1 class="c3 c5" id="h.oz1x1oon17xf"><span class="c1">Follow-Up Experiment</span></h1><p class="c3"><span class="c1">In an attempt to lower early cancellation, it might be interesting to introduce a feature where if the total time played on a video exceeds 3 times its length, a pop-up reminds the student there are forum information available. If possible, even filter the exact threads pertaining to the topic of the video. The null hypothesis would be that the student retention difference is equal to zero. It would be a two tail test to test for both positive or negative impacts. The evaluation metric would be the retention rate 30 days after first payment. It is reasonable to believe students that will end subscription in 30 days will be much less than students that ended free trial. Therefore, sample size problems should not exist as it did before, although we will not know for sure without some analytical estimates. User-id would be the unit of diversion, as each user-id is an unique paying student and makes sense as our denominator.</span></p><p class="c2"><span class="c1"></span></p></body></html>